# Types of Volumes
In Kubernetes, all containers are ephemeral and Kubernetes volume is an abstraction implemented to solve two problems.

Loss of files when a container crashes.
Sharing files between containers running together in a Pod.
The volumes fall under three major categories,

- Ephemeral Volumes
- Persistent Volumes
- Projected Volumes

## Ephemeral volumes

Ephemeral volumes follow the Pod's lifetime and get created and deleted along with the Pod, Pods can be stopped and restarted without being limited to where some persistent volume is available.

In the following example we are going to examine emptyDir ephemeral storage.

An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node.

The example shows two containers with an emptyDir shared volume.

First, the pod has a volume called shared-vol. This volume is mounted in the WebServer container at /var/www/html, because thatâ€™s the directory the web server serves files from.

The same volume is also mounted in the content-generator container, but at /data, because that's where the generator writes the files to.

By mounting this single volume like that, the web server will now serve the content generated by the content generator.

Apply the manifest file example:

```
cat <<'EOF' > web-server.yaml | kubectl apply -f web-server.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: web-server
  name: web-server

spec:
    volumes:
    - name: shared-vol
      emptyDir: {}

    initContainers:
    - name: content-generator
      image: busybox
      volumeMounts:
      - name: shared-vol
        mountPath: /data

      command: [ "/bin/sh", "-c" ]
      args: [ 'echo \"Website initialized successfully!\" > /data/index.html' ]

    containers:
    - image: httpd
      name: httpd
      volumeMounts:
      - name: shared-vol
        mountPath: /var/www/html/
EOF
```

Check the Pod status:

```
watch kubectl get pod web-server
```

After the Pod is in Running status, check the filesystem:

```
kubectl exec web-server -c httpd -- cat /var/www/html/index.html
```

The command will return the content of the index.html shared file

```
Website initialized successfully!
```

## A projected volume

A projected volume maps several existing volume sources into the same directory.

Currently, the following types of volume sources can be projected:

secret
downwardAPI
configMap
serviceAccountToken
In the following example we are going to configure a projected volume for a pod.

We have created a secret and a configmap object to use in this exercise.

You can get information about the objects created by running the following commands:

```
kubectl get secret mysecret
kubectl get configmap myconfigmap
```

Next, we are going to configure the Pod with these sources:

```
cat <<'EOF' > projected-volume.yaml | kubectl apply -f projected-volume.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    args:
    - /bin/sh
    - -c
    - sleep 3600
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
EOF
```

Check the Pod status:

```
watch kubectl get pod volume-test
```

After the Pod is in Running status, check the filesystem:

```
kubectl exec volume-test -- ls /projected-volume/my-group
kubectl exec volume-test -- cat /projected-volume/my-group/my-config
kubectl exec volume-test -- cat /projected-volume/my-group/my-username
```

## A Persistent Volume

A Persistent Volume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource and have a lifecycle independent of any individual Pod that uses the PV.

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes.

Let's create a PersistentVolume of 10Gi, called myvolume. With the following properties:

- accessMode of 'ReadWriteOnce'
- storageClassName 'local-path'
- mounted on hostPath '/etc/foo'

```
cat<<'EOF' > pv.yaml | kubectl apply -f pv.yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: myvolume
spec:
  storageClassName: local-path
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
    - ReadWriteMany
  hostPath:
    path: /etc/foo
EOF
```

Check the object status:

```
kubectl get pv myvolume
```

Next, create a PersistentVolumeClaim called mypvc with the following properties:

- a request of 4Gi
- accessMode of ReadWriteOnce
- with the storageClassName of local-path


```
cat<<'EOF' > pvc.yaml | kubectl apply -f pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mypvc
spec:
  storageClassName: local-path
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
EOF
```

Check the object status:

```
kubectl get pvc mypvc
```

Next, we are going to create a busybox pod and use the PersistentVolumeClaim we created.

```
cat <<'EOF'> busybox-pod-one.yaml | kubectl apply -f busybox-pod-one.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:

  volumes:
  - name:  my-vol # has to match volumeMounts.name
    persistentVolumeClaim:
      claimName: mypvc

  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}

    volumeMounts:
    - name: my-vol # has to match volumes.name
      mountPath: /etc/foo
EOF
```

Check the Pod status:

```
watch kubectl get pod busybox
```

Next, we are going to validate data persistence by connecting to the pod and copying '/etc/passwd' to '/etc/foo/passwd' then deleting it:

```
kubectl exec busybox -it -- cp /etc/passwd /etc/foo/passwd
```

delete the Pod:

```
kubectl delete pod busybox --force --grace-period 0
```

Next, we are going to create a second pod which is identical with the one we created and mount it to the same volume and validate the file we copied exists.

```
cat <<'EOF'> busybox-pod-two.yaml | kubectl apply -f busybox-pod-two.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox-two
  name: busybox-two
spec:

  volumes:
  - name:  my-vol # has to match volumeMounts.name
    persistentVolumeClaim:
      claimName: mypvc

  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}

    volumeMounts:
    - name: my-vol # has to match volumes.name
      mountPath: /etc/foo
EOF
```

Check the Pod status:

```
watch kubectl get pod busybox-two
```

Verify that '/etc/foo' contains the 'passwd' file.

```
kubectl exec busybox-two -- ls /etc/foo # will show 'passwd'
```

We are able to see the file we copied as data persisted in the Persistent Volume we created after we deleted the first Pod.
